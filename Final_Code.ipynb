{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit The Data\n",
    "\n",
    "In this section we will iterate over the street names and zip codes. We will check for uniformity in the data against an expected dictionary of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import csv\n",
    "import cerberus\n",
    "import schema\n",
    "import codecs\n",
    "\n",
    "osm_file = open(\"tampa_florida.osm\", \"r\")\n",
    "\n",
    "#\n",
    "\n",
    "#########################################\n",
    "\n",
    "# Search for all unexpected street types\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "\n",
    "# List of expected street name values\n",
    "expected = [\"Street\", \"Avenue\", \"Lane\", \"Way\", \"Boulevard\",  \n",
    "\"Drive\", \"Court\", \"Place\", \"Square\", \"Road\", \"Trail\", \"Parkway\", \"Commons\", \"North\", \"South\", \"West\", \"East\"]\n",
    "\n",
    "# Here we add any street name that isn't in our expected dictionary\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "    return street_types\n",
    "\n",
    "# Here we create a dictionary of our postal codes\n",
    "def audit_postal_code(postal_code_types, postal_code):  \n",
    "    if not postal_code.isupper() or ' ' not in postal_code:\n",
    "        postal_code_types['case_whitespace_problems'].add(postal_code)\n",
    "    else:\n",
    "        postal_code_types['other'].add(postal_code)\n",
    "    return postal_code_types\n",
    "\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "\treturn (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "audit(osm_file)\n",
    "#print_sorted_dict(street_types)\n",
    "\n",
    "def audit(filename):\n",
    "    f = (filename)\n",
    "    street_types = defaultdict(set)\n",
    "    postal_code_types = defaultdict(set)\n",
    "    \n",
    "    for event, element in ET.iterparse(f, events=(\"start\",)):\n",
    "        if element.tag == \"node\" or element.tag ==\"way\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(postal_code_types, tag.attrib['v'])\n",
    "                    \n",
    "    f.close()\n",
    "    return dict(street_types), dict(postal_code_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the audits we found that street names were very inconsistent. Often mixing abreviations with full spellings of words like \"street\" vs \"st.\", Postal codes were also problematic. Mixing 5 digit postal codes and 10 digit postal codes. Some even contained the state abbreviation within in them ex: 33709FL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Street Names and Postal Codes\n",
    "\n",
    "Having observed inconsistencies within the street name and zip code data we will create cleaning functions that will update the street names and zip codes according to a mapping standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"E\" : \"East\",\n",
    "            \"W\" : \"West\",\n",
    "            \"S\" : \"South\",\n",
    "            \"N\" : \"North\"\n",
    "            }\n",
    "\n",
    "# Here we iterate over each name and compare/update it in regards to our selected mapping\n",
    "def update_name(street_name, mapping):\n",
    "    street_name = street_name.replace(' ', ' ')\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_type2 = ' '.join(street_name.split()[0:])\n",
    "        if street_type in mapping.keys():\n",
    "            #print 'Before: ' , name\n",
    "            street_name = re.sub(street_type, mapping[street_type],street_name)\n",
    "            #print 'After: ', name\n",
    "        \n",
    "    return street_name\n",
    "\n",
    "# Here we iterate over the postal codes making them a uniform 5 digit number\n",
    "def update_postal_code(postal_code):\n",
    "    postal_code = postal_code.upper()\n",
    "    if ' ' not in postal_code:\n",
    "        if len(postal_code) != 5:\n",
    "            postal_code = postal_code[0:5]\n",
    "    return postal_code\n",
    "\n",
    "\n",
    "def street_test():\n",
    "    \n",
    "    st_types = audit(osm_file)[0]\n",
    "    print \"\"\n",
    "    for st_types, st_names in st_types.iteritems():\n",
    "        for name in st_names:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"->\", better_name\n",
    "            \n",
    "def postal_code_test():\n",
    "    postcode_types = audit(osm_file)[1]\n",
    "    #pprint(postcode_types)\n",
    "    print \"\"\n",
    "    \n",
    "    for postcode_type, postcodes in postcode_types.iteritems():\n",
    "        for postcode in postcodes:\n",
    "            better_postcode = update_postal_code(postcode)\n",
    "            print postcode, \"=>\", better_postcode\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM To CSV\n",
    "Here we will take the osm file and shape them into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_PATH = \"tampa_florida.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+\\/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "\n",
    "# The function 'make_tag_dict()' takes an element and a child tag, and then creates a dictionary with keys 'id', 'key', 'value', 'type'\n",
    "# according to the rules specified in the problem\n",
    "\n",
    "def make_tag_dict(element, tag):\n",
    "    tag_attribs = {}                           \n",
    "    tag_attribs['id'] = element.attrib['id']\n",
    "    \n",
    "    if is_street_name(tag):\n",
    "        tag_attribs['value'] = update_street_name(tag.attrib['v'], mapping, mapping2)  \n",
    "    elif is_postal_code(tag):\n",
    "        tag_attribs['value'] = update_postal_code(tag.attrib['v'])              # update street names and postal codes\n",
    "                                                                                \n",
    "    else:\n",
    "        tag_attribs['value'] = tag.attrib['v']\n",
    "    \n",
    "    k_attrib = tag.attrib['k']\n",
    "    if not PROBLEMCHARS.search(k_attrib):\n",
    "        if LOWER_COLON.search(k_attrib):        # If the 'k_attrib' string contains a ':' character, then set \n",
    "            key = k_attrib.split(':', 1)[1]     # tag_attribs['key'] to be everything after the first colon,\n",
    "            tipe = k_attrib.split(':', 1)[0]    # and tag_attribs['type'] to be everything before the first colon\n",
    "            tag_attribs['key'] = key\n",
    "            tag_attribs['type'] = tipe\n",
    "        else:\n",
    "            tag_attribs['key'] = k_attrib\n",
    "            tag_attribs['type'] = 'regular'\n",
    "        \n",
    "    return tag_attribs\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for item in NODE_FIELDS:                      # Populate the 'node_attribs' dict with the keys from NODE_FIELDS\n",
    "            node_attribs[item] = element.attrib[item] # and the values from the 'element.attrib' dictionary\n",
    "        for tag in element.iter('tag'):\n",
    "            if tag.attrib['v'] == \"\" or tag.attrib['v'] == None:\n",
    "                continue\n",
    "            tag_attribs = make_tag_dict(element, tag) # Call the function make_tag_dict() that creates a dictionary of\n",
    "            tags.append(tag_attribs)                  # tag attributes.  Then append this dict to the 'tags' list.\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for item in WAY_FIELDS:                       # Populate the 'way_attribs' dict with the keys from WAY_FIELDS\n",
    "            way_attribs[item] = element.attrib[item]  # and the values from the 'element.attrib' dict\n",
    "        for tag in element.iter('tag'):\n",
    "            if tag.attrib['v'] == \"\" or tag.attrib['v'] == None:\n",
    "                continue\n",
    "            tag_attribs = make_tag_dict(element, tag) # Again use the function make_tag_dict() to create a dictionary \n",
    "            tags.append(tag_attribs)                  # of tag attributes\n",
    "            \n",
    "        position = 0\n",
    "        for tag in element.iter('nd'):\n",
    "            nd_attribs = {}                           # Initialize and populate the 'nd_attribs' dictionary according\n",
    "            nd_attribs['id'] = element.attrib['id']   # to the rules specified in the problem\n",
    "            nd_attribs['node_id'] = tag.attrib['ref']\n",
    "            nd_attribs['position'] = position\n",
    "            position += 1\n",
    "            way_nodes.append(nd_attribs)\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "\n",
    "# ================================================== #\n",
    "#              Other Helper Functions                #\n",
    "# ================================================== #\n",
    "\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV to Database file\n",
    "Here we will take the CSV file created above and create a Database file that we can finally load into SQL and create queries for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class dbSQL(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "                      \n",
    "        self.connection = sqlite3.connect('tampa_florida.db') \n",
    "            \n",
    "    def create_table(self, table_name, table_schema):\n",
    "        \n",
    "        t = self.connection.cursor()\n",
    "        t.execute(table_schema)\n",
    "        self.connection.commit()\n",
    "        \n",
    "    def insert_data(self, table_name, csv_file):\n",
    "        self.connection.text_factory = lambda x: x.decode('latin-1')\n",
    "\n",
    "        df_nodes = pd.read_csv(csv_file)\n",
    "        df_nodes.to_sql(table_name, self.connection, if_exists='append', index=False)\n",
    "        \n",
    "    def close_connection(self):\n",
    "        self.connection.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database inqueries\n",
    "\n",
    "Below we have our data base queries to help explore and analyze the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "from pprint import pprint\n",
    "\n",
    "database_file = 'tampa_florida.db'\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def postal_code_counts():\n",
    "    query = \"select subq.value, count(*) as count from (select * from nodes_tags union select * from ways_tags) subq where subq.key = 'postcode' group by subq.value order by count desc\"\n",
    "    cursor.execute(query)\n",
    "    results =cursor.fetchall()\n",
    "    \n",
    "    pprint(results)\n",
    "    \n",
    "def user_count():\n",
    "    query = \"select count(sub.uid) from (select uid from nodes union select uid from ways) sub\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "\n",
    "    \n",
    "def nodes_count():\n",
    "    query = \"select count(*) from nodes\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "\n",
    "\n",
    "def ways_count():\n",
    "    query = \"select count(*) from ways\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "    \n",
    "    \n",
    "def get_places():\n",
    "    query = \"select sub.value, nt.value from nodes_tags nt join (select id, value from nodes_tags where key = 'place') sub on nt.id = sub.id where nt.key = 'name' limit 10\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "    \n",
    "\n",
    "def place_count():\n",
    "    query = \"select value, count(*) from nodes_tags where key = 'place' group by value\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "    \n",
    "    \n",
    "def amenities_count():\n",
    "    df = pd.read_sql_query(\"select value, count(*) as num from nodes_tags where key = 'amenity' group by value order by num desc limit 10;\", conn)\n",
    "    print df\n",
    "    \n",
    "    \n",
    "def cuisines():\n",
    "    query = \"SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') i ON nodes_tags.id=i.id WHERE nodes_tags.key='cuisine' GROUP BY nodes_tags.value ORDER BY num DESC;\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "    \n",
    "    \n",
    "def postal_code_counts():\n",
    "    query = \"select subq.value, count(*) as count from (select * from nodes_tags union select * from ways_tags) subq where subq.key = 'postcode' group by subq.value order by count desc\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    pprint(results)\n",
    "    \n",
    "def top_users():\n",
    "    df = pd.read_sql_query(\"Select user, COUNT(user) as num FROM nodes GROUP BY uid ORDER by num DESC;\", conn)\n",
    "    \n",
    "    print df[0:10]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
